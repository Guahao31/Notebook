# 补天记录

```
一些补天记录，主要涉及 Transformer & Post-Training 原理（直观理解和公示结果为主）与对应代码记录（甚至不是解读啊喂）；针对大模型训练/推理/后训练过程的各种系统优化与代码记录。

这里的所有内容充斥着脑补、“俺寻思”和 AI 幻觉，不足以作为学习资料，纯粹个人发癫记录。
```

## Transformer

Transformer 是当前最流行的大模型基本结构，个人认为需要补充学习其基本原理和运算过程（从算法+ infra 角度）。初步计划进行下面这些内容的学习：Attention（从基础的 MHA 到 GQA、MQA）、RoPE、FFN(SwiGLU)、代码阅读（可能 llama2.c）。

### Attention - MHA

先放公式，对于一个简单的“单头”注意力来说，它的基本运算是 $\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^{T}}{\sqrt{D_{h}}})V$。下面直观理解一下注意力机制的运算过程：

- 首先需要对 $Q, K, V$ 这三个张量本身有个直观理解
    - $Q$ 即 query，可以直观理解为“我当前正在读的这个词，我想找什么相关的信息”
    - $K$ 即 key，可以直观理解为“我有什么特征，能被别人查询到”
    - $V$ 即 value，可以直观理解为“如果我和你的查询匹配了，我应该把什么信息传给你”
    ```
    给出一个例子来辅助理解：如果你在图书馆找书，那么你的书单就是 `Q`，它上边可能写着“关于大模型训练的书”；书架上的每一本书都有分类标签，比如“计算机系统”、“变形金刚”、“AI Infra”、“炒饭”，这个就是 `K`；你会对着书单计算和每一本书的相似度，找到匹配度高的书，你会把它抽出来阅读内容，这个内容就是这本书的 `V`。
    ```
- 在之后的讨论中，每个张量的维度都很重要，因此顺便在过程中标注了，假设我们一批处理 `B(atch)` 组向量，其中每个向量的大小都是 `[S(equence), D(imension)_h(ead)]`，至于这个大小有什么含义，暂时按下不表
- $QK^T$: $[B, S, D_h] \cdot [B, S, D_h]^T \rightarrow [B, S, S]$
    - 注意其中的转置针对的是后两维，即批内每个向量进行一次 $[S, D_h] \cdot [D_h, S]$ 的矩阵乘
    - 它的含义可以直观理解为，进行例子中的“匹配书单与书的标签相似度”的动作，得到的结果中每一行都是一条要求对另一本书的相似度
    - 需要特别关注这个中间结果的维度是 $[S, S]$，当我处理的序列长度增长时，这部分的显存占用会呈平方增长，这与后边计算过程中的显存优化有很大的关系
    - 得到的结果被称为分数矩阵(Attention Scores)
- $\frac{\cdot}{\sqrt{D_h}}$: $[B, S, S] \rightarrow [B, S, S]$
