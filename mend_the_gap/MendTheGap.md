# 补天记录

```
一些补天记录，主要涉及 Transformer & Post-Training 原理（直观理解和公示结果为主）与对应代码记录（甚至不是解读啊喂）；针对大模型训练/推理/后训练过程的各种系统优化与代码记录。

这里的所有内容充斥着脑补、“俺寻思”和 AI 幻觉，不足以作为学习资料，纯粹个人发癫记录。
```

## Transformer

Transformer 是当前最流行的大模型基本结构，个人认为需要补充学习其基本原理和运算过程（从算法+ infra 角度）。初步计划进行下面这些内容的学习：Attention（从基础的 MHA 到 GQA、MQA）、RoPE、FFN(SwiGLU)、代码阅读（可能 llama2.c）。

### Attention - MHA

先放公式，对于一个简单的“单头”注意力来说，它的基本运算是 $\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^{T}}{\sqrt{D_{h}}})V$。下面直观理解一下注意力机制的运算过程：

- 首先需要对 $Q, K, V$ 这三个张量本身有个直观理解
    - $Q$ 即 query，可以直观理解为“我当前正在读的这个词，我想找什么相关的信息”
    - $K$ 即 key，可以直观理解为“我有什么特征，能被别人查询到”
    - $V$ 即 value，可以直观理解为“如果我和你的查询匹配了，我应该把什么信息传给你”
    ```
    给出一个例子来辅助理解：如果你在图书馆找书，那么你的书单就是 `Q`，它上边可能写着“关于大模型训练的书”；书架上的每一本书都有分类标签，比如“计算机系统”、“变形金刚”、“AI Infra”、“炒饭”，这个就是 `K`；你会对着书单计算和每一本书的相似度，找到匹配度高的书，你会把它抽出来阅读内容，这个内容就是这本书的 `V`。
    ```
- 在之后的讨论中，每个张量的维度都很重要，因此顺便在过程中标注了，假设我们一批处理 `B(atch)` 组向量，其中每个向量的大小都是 `[S(equence), D(imension)_h(ead)]`，至于这个大小有什么含义，暂时按下不表
- 先获取 $Q, K, V$，$Q=X \cdot W_Q, K = X \cdot W_K, V = X \cdot W_V$
    - $X$ 代表输入，其维度为 $B, S, D_h$
    - $W\_$ 表示对应的权重矩阵，其维度为 $D_h, D_h$
    - 在进行矩阵乘法时，会进行权重矩阵的广播，最终结果的维度是 $B, S, D_h$
- $QK^T$: $[B, S, D_h] \cdot [B, S, D_h]^T \rightarrow [B, S, S]$
    - 注意其中的转置针对的是后两维，即批内每个向量进行一次 $[S, D_h] \cdot [D_h, S]$ 的矩阵乘
    - 它的含义可以直观理解为，进行例子中的“匹配书单与书的标签相似度”的动作，得到的结果中每一行都是一条要求对另一本书的相似度
    - 需要特别关注这个中间结果的维度是 $[S, S]$，当我处理的序列长度增长时，这部分的显存占用会呈平方增长，这与后边计算过程中的显存优化有很大的关系
    - 得到的结果被称为分数矩阵(Attention Scores)
- $\frac{\cdot}{\sqrt{D_h}}$: $[B, S, S] \rightarrow [B, S, S]$
    - 在得到 $QK^T$ 的点积结果后，需要对这个结果进行一次缩放，缩放的比例是 $\sqrt{D_h}$ 即单头维度的根号
      - 一个很自然的问题是，为什么需要这次缩放？首先点积的计算过程是 $D_h$ 个乘积之和，可能导致计算结果中绝对值的差别很大（点积结果的方差变大），而 Sotfmax 的操作对数值大小很敏感，可能导致最终结果的概率值极其偏向（几乎非 0 即 1），在反向过程中，这样的分布会导致梯度几乎是 0，无法对参数 $W_Q, W_K$ 进行有效的更新
      - 为什么要除的是这个值？因为点积过程可以描述为 $\sum_{i=1}^{D_h}(q_i \cdot k_i)$，当 $q_i, k_i$ 都是随机变量（假设均值为 0 方差为 1），那么 $q_i \cdot k_i$ 的方差约等于 1，（因为每一维上的数据都是独立的，所以方差可加）点积结果的方差为 $D_h$，当我们对点积结果进行除 $\sqrt{D_h}$ （标准差的值）后，其方差被拉回到 1，以此保持数据稳定
- $\text{softmax}(\cdot)$: $[B, S, S] \rightarrow [B, S, S]$
    - 对每一行做 softmax，其公式为 $\text{softmax}(x) = \frac{e^{x_i}}{\sum{e^{x_j}}}$，可以发现计算后每一行的元素之和为 1，完成了一次归一化（将数值转换为了概率值）
- $\cdot V$: $[B, S, S] \cdot [B, S, D_h] \rightarrow [B, S, D_h]$
    - 加权求和，得出本层的结果，同时这一层的维度也恢复到了输入的维度大小

到这里已经对单头注意力有了基本的理解，现在我们将其拓展为“多头注意力”（Multi-Head Attention，MHA），简单理解就是同时搞出很多个注意力头，每个头都有“各自更关注的信息”，比如一个头专门注意上一个 token，一个头专门注意语法，一个头专门注意专有名词。（当然这个例子只是用来说明不同头关注的东西不同，并不是说我们能保证它们在分别关注我们设计的内容）

不同的头是并行进行计算的，后面的说明中我们用 `D_m(odel) i.e. total hidden size` 来指代所有头的维度之和，用 `D_h(ead)` 表示单头维度，它们的关系是 $D_m = H \times D_h$ 其中 $H$ 是注意力头数。

多头注意力的计算流程与单头注意力几乎相同，它通常将所有权重矩阵合并为 $[D_m, D_m]$，线性投影后得到 $[B, S, D_m]$ 的 $Q, K, V$；之后进行拆分和转置产生“多头”，会先将最后一维的 $D_m$ 拆分为 $H \times D_h$ 得到 $[B, S, H, D_h]$，交换维度提前“头”这一维，保证后续计算的显存连续，得到 $[B, H, S, D_h]$，之后不同头如单头注意力一般并行的分别进行注意力计算，进行转置和合并，得到输出 $[B, S, D_m]$，这个输出再与 $W_O$(Output Projection, $[D_m, D_m]$) 相乘，混合特征得到最终结果。与单头注意力相比，MHA 引入了一个可训练的权重 $W_O$，其作用是将多头的处理特征进行融合，得到最终的结果。

综上，可以将 MHA 的运算总结为：

$$MultiHeadAttention(Q, K, V) = Concat(head_1, head_2, \dots, head_h)W^O$$

其中，$$head_i = Attention(QW^{Q}_{i}, KW^{K}_{i}, VW^{V}_{i}), W^{\cdot}_{i} \in \mathbb{R}^{D_m \times D_h}$$

这里自然产生了两个问题：在 $D_m$ 固定的前提下，调整 $H$ 和 $D_h$ 有什么用处？如何保证不同头“关注”不同的特性？

因为限制 $D_m = H \times D_h$，头数和每头维度是呈反比的， 从算法的角度看，多头可能可以捕捉更多类型的特征关系，而少头的方案每个向量的信息更多，能够容纳更精细的信息；从计算角度看，在每个头分别进行注意力计算的时候，其计算矩阵大小是和 $D_h$ 相关的，而过小过多的 GPU 计算会带来开销。因此这是一个 trade-off，后边 FlashAttention 的学习可能能够补齐这部分内容，即如何选择 $D_h$ 的大小。

不同头关注不同信息，是比较玄学的说法，在模型初始化阶段，它们的权重矩阵是随机的，不同头的权重初始值并不相同，这在最开始保证了不同头一定是不一样的；而在训练过程中，可能通过 loss 的设计，要求每个头向其他方向发展，这部分尚待学习，没有完全理解。

我们可以发现，在 MHA 的设计下，每个头都有自己独立的 $K$ 和 $V$，如果每个头其实关注的东西都差不多，那么它们是不是可以进行合并？这就是后边的 MQA 和 GQA 的设计关注点。
