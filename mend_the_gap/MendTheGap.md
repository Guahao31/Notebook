# 补天记录

```
一些补天记录，主要涉及 Transformer & Post-Training 原理（直观理解和公示结果为主）与对应代码记录（甚至不是解读啊喂）；针对大模型训练/推理/后训练过程的各种系统优化与代码记录。

这里的所有内容充斥着脑补、“俺寻思”和 AI 幻觉，不足以作为学习资料，纯粹个人发癫记录。
```

## Transformer

Transformer 是当前最流行的大模型基本结构，个人认为需要补充学习其基本原理和运算过程（从算法+ infra 角度）。初步计划进行下面这些内容的学习：Attention（从基础的 MHA 到 GQA、MQA）、RoPE、FFN(SwiGLU)、代码阅读（可能 llama2.c）。

### Attention - MHA

先放公式，对于一个简单的“单头”注意力来说，它的基本运算是 $\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^{T}}{\sqrt{D_{h}}})V$。下面直观理解一下注意力机制的运算过程：

- 首先需要对 $Q, K, V$ 这三个张量本身有个直观理解
    - $Q$ 即 query，可以直观理解为“我当前正在读的这个词，我想找什么相关的信息”
    - $K$ 即 key，可以直观理解为“我有什么特征，能被别人查询到”
    - $V$ 即 value，可以直观理解为“如果我和你的查询匹配了，我应该把什么信息传给你”
    ```
    给出一个例子来辅助理解：如果你在图书馆找书，那么你的书单就是 `Q`，它上边可能写着“关于大模型训练的书”；书架上的每一本书都有分类标签，比如“计算机系统”、“变形金刚”、“AI Infra”、“炒饭”，这个就是 `K`；你会对着书单计算和每一本书的相似度，找到匹配度高的书，你会把它抽出来阅读内容，这个内容就是这本书的 `V`。
    ```
- 在之后的讨论中，每个张量的维度都很重要，因此顺便在过程中标注了，假设我们一批处理 `B(atch)` 组向量，其中每个向量的大小都是 `[S(equence), D(imension)_h(ead)]`，至于这个大小有什么含义，暂时按下不表
- 先获取 $Q, K, V$，$Q=X \cdot W_Q, K = X \cdot W_K, V = X \cdot W_V$
    - $X$ 代表输入，其维度为 $B, S, D_h$
    - $W\_$ 表示对应的权重矩阵，其维度为 $D_h, D_h$
    - 在进行矩阵乘法时，会进行权重矩阵的广播，最终结果的维度是 $B, S, D_h$
- $QK^T$: $[B, S, D_h] \cdot [B, S, D_h]^T \rightarrow [B, S, S]$
    - 注意其中的转置针对的是后两维，即批内每个向量进行一次 $[S, D_h] \cdot [D_h, S]$ 的矩阵乘
    - 它的含义可以直观理解为，进行例子中的“匹配书单与书的标签相似度”的动作，得到的结果中每一行都是一条要求对另一本书的相似度
    - 需要特别关注这个中间结果的维度是 $[S, S]$，当我处理的序列长度增长时，这部分的显存占用会呈平方增长，这与后边计算过程中的显存优化有很大的关系
    - 得到的结果被称为分数矩阵(Attention Scores)
- $\frac{\cdot}{\sqrt{D_h}}$: $[B, S, S] \rightarrow [B, S, S]$
    - 在得到 $QK^T$ 的点积结果后，需要对这个结果进行一次缩放，缩放的比例是 $\sqrt{D_h}$ 即单头维度的根号
      - 一个很自然的问题是，为什么需要这次缩放？首先点积的计算过程是 $D_h$ 个乘积之和，可能导致计算结果中绝对值的差别很大（点积结果的方差变大），而 Sotfmax 的操作对数值大小很敏感，可能导致最终结果的概率值极其偏向（几乎非 0 即 1），在反向过程中，这样的分布会导致梯度几乎是 0，无法对参数 $W_Q, W_K$ 进行有效的更新
      - 为什么要除的是这个值？因为点积过程可以描述为 $\sum_{i=1}^{D_h}(q_i \cdot k_i)$，当 $q_i, k_i$ 都是随机变量（假设均值为 0 方差为 1），那么 $q_i \cdot k_i$ 的方差约等于 1，（因为每一维上的数据都是独立的，所以方差可加）点积结果的方差为 $D_h$，当我们对点积结果进行除 $\sqrt{D_h}$ （标准差的值）后，其方差被拉回到 1，以此保持数据稳定
- $\text{softmax}(\cdot)$: $[B, S, S] \rightarrow [B, S, S]$
    - 对每一行做 softmax，其公式为 $\text{softmax}(x) = \frac{e^{x_i}}{\sum{e^{x_j}}}$，可以发现计算后每一行的元素之和为 1，完成了一次归一化（将数值转换为了概率值）
- $\cdot V$: $[B, S, S] \cdot [B, S, D_h] \rightarrow [B, S, D_h]$
    - 加权求和，得出本层的结果，同时这一层的维度也恢复到了输入的维度大小

到这里已经对单头注意力有了基本的理解，现在我们将其拓展为“多头注意力”（Multi-Head Attention，MHA），简单理解就是同时搞出很多个注意力头，每个头都有“各自更关注的信息”，比如一个头专门注意上一个 token，一个头专门注意语法，一个头专门注意专有名词。（当然这个例子只是用来说明不同头关注的东西不同，并不是说我们能保证它们在分别关注我们设计的内容）

不同的头是并行进行计算的，后面的说明中我们用 `D_m(odel) i.e. total hidden size` 来指代所有头的维度之和，用 `D_h(ead)` 表示单头维度，它们的关系是 $D_m = H \times D_h$ 其中 $H$ 是注意力头数。

多头注意力的计算流程与单头注意力几乎相同，它通常将所有权重矩阵合并为 $[D_m, D_m]$，线性投影后得到 $[B, S, D_m]$ 的 $Q, K, V$；之后进行拆分和转置产生“多头”，会先将最后一维的 $D_m$ 拆分为 $H \times D_h$ 得到 $[B, S, H, D_h]$，交换维度提前“头”这一维，保证后续计算的显存连续，得到 $[B, H, S, D_h]$，之后不同头如单头注意力一般并行的分别进行注意力计算，进行转置和合并，得到输出 $[B, S, D_m]$，这个输出再与 $W_O$(Output Projection, $[D_m, D_m]$) 相乘，混合特征得到最终结果。与单头注意力相比，MHA 引入了一个可训练的权重 $W_O$，其作用是将多头的处理特征进行融合，得到最终的结果。

综上，可以将 MHA 的运算总结为：

$$MultiHeadAttention(Q, K, V) = Concat(head_1, head_2, \dots, head_h)W^O$$

其中，$$head_i = Attention(QW^{Q}_{i}, KW^{K}_{i}, VW^{V}_{i}), W^{\cdot}_{i} \in \mathbb{R}^{D_m \times D_h}$$

这里自然产生了两个问题：在 $D_m$ 固定的前提下，调整 $H$ 和 $D_h$ 有什么用处？如何保证不同头“关注”不同的特性？

因为限制 $D_m = H \times D_h$，头数和每头维度是呈反比的， 从算法的角度看，多头可能可以捕捉更多类型的特征关系，而少头的方案每个向量的信息更多，能够容纳更精细的信息；从计算角度看，在每个头分别进行注意力计算的时候，其计算矩阵大小是和 $D_h$ 相关的，而过小过多的 GPU 计算会带来开销。因此这是一个 trade-off，后边 FlashAttention 的学习可能能够补齐这部分内容，即如何选择 $D_h$ 的大小。

不同头关注不同信息，是比较玄学的说法，在模型初始化阶段，它们的权重矩阵是随机的，不同头的权重初始值并不相同，这在最开始保证了不同头一定是不一样的；而在训练过程中，可能通过 loss 的设计，要求每个头向其他方向发展，这部分尚待学习，没有完全理解。

我们可以发现，在 MHA 的设计下，每个头都有自己独立的 $K$ 和 $V$，如果每个头其实关注的东西都差不多，那么它们是不是可以进行合并？这就是后边的 MQA 和 GQA 的设计关注点。

### Attention - MQA

由于 MHA 的处理中，$Q, K, V$ 都有独立的注意力头来分别计算，整个计算过程涉及到大量的访存，（尤其是在推理的 decoding 阶段）将这个计算过程从“计算密集型”向“访存密集型”转移。Google 在 19 年的论文中提出了 MQA(Mutlti-Query Attention) 来解决这个问题，设计的整体思路就是仅保留 $Q$ 的多头，而共用一头的 $K, V$，这样的设计在保留多头注意力整合不同关注的同时，将 decode 过程从 MHA 的访存密集型转移回计算密集型，充分利用 GPU 的计算能力（而不至于被访存卡死）。

首先简介 MQA 的基本流程，与 MHA 几乎相同，其唯一的差别在于，MHA 的运算过程中 $K, V$ 都是由多个注意力头进行运算处理的（其 shape 在计算过程中为 $[B, S, H, D_h]$），而在 MQA 的设计下，仅 $Q$ 的大小不变，$K, V$ 均只保留一个头（即 shape 为 $[B, S, 1, D_h]$）。（之后在推理流程的学习里可能对这部分内容再做补充）

第一个问题是，MQA 仅仅将 $K, V$ 从多头改为单头，怎么能“将 decoding 阶段 MHA 的访存密集型转变为计算密集型”？关键在于对 GPU 访问 HBM 的总数据量和 GPU Cache 命中率上进行分析。在 MHA 的计算过程中，一次 Attention 运算需要访问 $[B, S, H, D_h]$ 的 $Q, K, V$，而它们的每个维度都各不相同，无法在 GPU cache 上形成命中，对 HBM 的访存量极大。而在 MQA 的设计下，除了 $Q$ 还是访问原来的大小之外，$K, V$ 在运算过程中访存行为的链条就短了许多，它们本身的大小是 $[B, S, 1, D_h]$ 而在运算过程中会**广播**到 $[B, S, H, D_h]$ 的大小参与运算，需要注意这里的广播并不是物理上将它们拷贝 $H$ 份，而是设计一个逻辑上的访问限制，即进行了一次地址映射，比如访问第 3 个头的向量时，实际上还是被映射到了仅有的那个头对应的向量上，而 MQA 下 Q 的多头注意力并行计算时访存行为几乎相同，因此能够命中还在 GPU Cache 上的 $K, V$ 向量，而不需要从 HBM 中获得，这将 Attention 的访存压力下降，能够更加充分的利用 GPU 的计算。

第二个问题是，为什么刚才一直强调是 decoding 阶段，而不说推理的 encoding(prefill) 阶段？因为 encoding 阶段 token 是一股脑送进来的，它本身并不伴随 KV cache 的压力以及转变为访存密集型的压力（因为可以合并为大矩阵运算，充分利用计算能力），而 decoding 阶段一次只处理一个 token，它不能满足 GPU 的计算能力，进而转变为了访存密集。

第三个问题是，共用单头 $K, V$ 是否会对效果有极大影响？初步结论是会有影响，但不大，几乎可以通过多训几轮来弥补。MHA 下实际上会存在“多头关注相同特征”的可能性，这个冗余性意味着可以对“多头”进行压缩，而 MQA 就是其中比较极限的压缩方式。而在压缩的前提下，MQA 还保留了 $Q$ 的多头特性，通过 query 的多头来“领导”对多样特征的关注，对训练过程没有特别严重的影响。同时，在 MHA 和 MQA 中都有一个 $W_O$ 对多头的特征进行融合，这可以看作是一种兜底，能够一定程度上弥补 MQA 对 KV 压缩带来的能力损失。

总结来看，MQA 利用 MHA 的冗余性进行压缩，减少对显存的访问（增加 cache 命中率）。对于推理过程，在 encoding 过程没有明显提升，但对于 decoding 阶段有极大提升。其最大优势是提升了推理阶段的吞吐和降低了 decoding 阶段的延迟；而对 encoding 阶段没有特别的好处。

### Attention - GQA

MHA 显存占用多、访存压力大，而 MQA 则面临 KV 存储信息过少实际效果较差的问题，GQA(Grouped Query Attention) 则是两个方案的折中。它保留了 MHA 和 MQA 对 $Q$ 的处理，即拥有完整的头数，而将 $Q$ 的不同头进行分组，每个组对应一个 $K, V$ 头。比如 $Q$ 有 $H_Q$ 头，分为了 $G$ 组，则 $K, V$ 就各有 $H_{\{K, V\}} = H_Q \div G$ 头。举个例子，$H_Q=32, G=8$ 时，每 4 个 Q 对应一组 K/V，即在运算过程中 $Q_{0\dots 3}, Q_{4\dots 7}\dots Q_{28\dots 31}$ 分别对应 $K_0(V_0), K_1(V_1), \dots, K_7(V_7)$。

GQA 实际上是对模型进行了一种结构化的剪枝，要求模型在训练时将相似的信息压缩到同一个 KV 组里，避免冗余性对模型过多的影响，同时减少仅有一组 KV 带来的信息丢失或处理能力下降。

与 MQA 类似，GQA 也可以将 Attention 计算过程中对 $K, V$ 的访存大小降低，区别在于 MQA 会降低 $H$ 倍，而 GQA 会降低 $G$ 倍，因此 GQA 也可以加速 decoding 阶段，而相比于 MQA 对模型质量有了进一步的保证。

至此，基本的几个 Attention 概念已经了解完全（MHA、MQA、GQA），可以对他们进行一定总结如下：

|   特性   	| MHA(Multi-Head) 	| GQA(Grouped-Query) 	| MQA(Multi-Query) 	|
|:--------:	|:---------------:	|:------------------:	|:----------------:	|
|  Q 头数  	|      H(32)      	|        H(32)       	|       H(32)      	|
|  KV 头数 	|      H(32)      	|        G(4)        	|         1        	|
| 显存占用 	|     高(100%)    	|     中(~41.67%)    	|    低(~35.42%)   	|
| 推理速度 	|  慢(I/O bound)  	|         快         	|       极快       	|
| 模型效果 	|       最好      	|      几乎无损      	|    有明显损失    	|

## Tokenizer & Embedding Layers

```
之前一直以为 Tokenizer 是个很神秘的东西。。。
```

Tokenizer 是数据准备&模型输入输出阶段需要使用的组件，它在数据输入层主要负责做这样的动作：`原始字符串(string) -> 分词后得到 token list（内容都为 string） -> 查表将 token 从字符串转换为数字`，即完成了一个从连续字符串到离散的 token 索引列表的动作。Tokenizer 需要关注的点如下：

- Tokenizer 的大小主要取决于词表(Vocabulary)大小 Vocab Size($V$)，它的大小会影响模型第一层和最后一层的大小（至于是什么后边会介绍）
- Tokenizer 本身在大模型训练过程中是不训练的（即不发生改变），它本身就是一个 token 到索引的映射表，其建立过程在大模型训练开始之前它的大小会影响模型第一层和最后一层的大小（至于是什么后边会介绍）
- Tokenizer（dataloader）通常是在 CPU 上完成的
- 训练过程中，为了把不同长度的句子放到同一个 Batch 里并行计算，会需要做 padding，用一个约定好的数值（比如 0）补齐短句子生成的 token list，而在 attention 运算是传入一个 mask，保证这些 padding 得到的分数一定是不会被选中的（比如负无穷）

可以看到，经过 tokenizer 的操作，我们将一段纯文本变为了一个整数列表，而我们在实际训练（比如之前的 attention 计算）中一直在处理“浮点数”，它们之间的关联是什么？

这个问题由 Embedding 层的引入来解决，大模型的第一层通常是对输入进行 Embedding，所谓的 embedding 就是将 token 进行向量化，从正数索引变为一个向量，经过输入 embedding 之后，我们的字符串就会变为一个矩阵了，而这个矩阵的获得就是由一个 token 查表得到的浮点向量构成。这个矩阵还需要引入“位置”的概念之后，送到后续层参与计算，这个位置编码就是后边学习的内容（比如 RoPE 来解决的）。在所有中间层计算结束后，我们会得到一个最终的结果向量，将这个结果向量和最后输出 embedding 层（或者叫 `lm_head`）相乘，获得一个和词法表大小相同的向量，这个向量代表着接下来输出每个 token 的概率，经过筛选后得到索引，并用这个索引从 tokenizer 查表得到最终的输出字符串。

输入/输出 embedding 层就是之前所说和词法表大小相关的层，它们的大小都是 $[V, D_m]$。需要注意，它们在大模型训练过程中都是可训练的。

至此我们可以理清数据格式的变化：`text(string) --tokenizer--> interger list --InputEmbedding--> X matrix --HiddenLayers--> Output Vector --OutputEmbedding--> Float vector --softmax--> index of output token --detokenizer--> output string`。
